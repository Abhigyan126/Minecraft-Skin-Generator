{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10607544,"sourceType":"datasetVersion","datasetId":6566528},{"sourceId":244938,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":209259,"modelId":230946},{"sourceId":245393,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":209663,"modelId":231355}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom PIL import Image\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\n\n\nclass MinecraftImageDataset(Dataset):\n    def __init__(self, folder_path, img_size=64):\n        \"\"\"\n        Custom Dataset to load valid images from a directory.\n        Args:\n            folder_path (str): Path to the folder containing images.\n            img_size (int): Target size to resize images (img_size x img_size).\n        \"\"\"\n        self.folder_path = folder_path\n        self.image_paths = self._filter_valid_images(folder_path)\n        self.transform = transforms.Compose([\n            transforms.Resize((img_size, img_size)),  # Resize images\n            transforms.ToTensor(),                    # Convert to tensor\n            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n        ])\n\n        print(f\"Total valid images: {len(self.image_paths)}\")  # Print the number of valid images\n\n    def _filter_valid_images(self, folder_path):\n        \"\"\"\n        Filters out invalid or corrupted images from the directory.\n        Args:\n            folder_path (str): Path to the folder containing images.\n\n        Returns:\n            List[str]: List of valid image paths.\n        \"\"\"\n        valid_images = []\n        for img in os.listdir(folder_path):\n            if img.endswith(('.png', '.jpg', '.jpeg')):  # Check for valid extensions\n                img_path = os.path.join(folder_path, img)\n                try:\n                    # Attempt to open and verify the image\n                    with Image.open(img_path) as image:\n                        image.verify()  # Check if the image is valid\n                    valid_images.append(img_path)\n                except Exception as e:\n                    print(f\"Invalid image: {img_path} - {e}\")  # Log invalid image details\n        return valid_images\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        image = Image.open(img_path).convert('RGB')  # Ensure 3-channel RGB\n        return self.transform(image)  # Apply transformations\n\n\ndef load_minecraft_data(folder_path, img_size=64, batch_size=32, shuffle=True):\n    \"\"\"\n    Creates a DataLoader for the Minecraft images with validation.\n    Args:\n        folder_path (str): Path to the folder containing images.\n        img_size (int): Target size to resize images (img_size x img_size).\n        batch_size (int): Number of images per batch.\n        shuffle (bool): Whether to shuffle the dataset.\n\n    Returns:\n        DataLoader: Torch DataLoader for the dataset.\n    \"\"\"\n    dataset = MinecraftImageDataset(folder_path, img_size=img_size)\n    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-29T16:11:43.171738Z","iopub.execute_input":"2025-01-29T16:11:43.172035Z","iopub.status.idle":"2025-01-29T16:11:50.374130Z","shell.execute_reply.started":"2025-01-29T16:11:43.172012Z","shell.execute_reply":"2025-01-29T16:11:50.373455Z"},"editable":false},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n\nimport torch.nn as nn\nimport torch\nimport torch.optim as optim\nfrom torchvision.utils import save_image, make_grid\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport torch.nn.functional as F\nimport numpy as np\nfrom skimage.metrics import structural_similarity as ssim_sklearn\n\n\n# Generator Model\nclass SelfAttention(nn.Module):\n    def __init__(self, in_channels):\n        super(SelfAttention, self).__init__()\n        self.query = nn.Conv2d(in_channels, in_channels // 8, 1)\n        self.key = nn.Conv2d(in_channels, in_channels // 8, 1)\n        self.value = nn.Conv2d(in_channels, in_channels, 1)\n        self.gamma = nn.Parameter(torch.zeros(1))\n\n    def forward(self, x):\n        batch_size, channels, height, width = x.size()\n        \n        proj_query = self.query(x).view(batch_size, -1, height * width).permute(0, 2, 1)\n        proj_key = self.key(x).view(batch_size, -1, height * width)\n        energy = torch.bmm(proj_query, proj_key)\n        attention = F.softmax(energy, dim=-1)\n        \n        proj_value = self.value(x).view(batch_size, -1, height * width)\n        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n        out = out.view(batch_size, channels, height, width)\n        \n        return self.gamma * out + x\n\nclass AdaptiveResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, use_spectral=True):\n        super(AdaptiveResidualBlock, self).__init__()\n        self.activation = nn.LeakyReLU(0.2, inplace=True)\n        \n        # Conditional batch norm or instance norm based on input size\n        self.norm1 = nn.BatchNorm2d(out_channels)\n        self.norm2 = nn.BatchNorm2d(out_channels)\n        \n        # Optional spectral normalization\n        if use_spectral:\n            self.conv1 = nn.utils.spectral_norm(\n                nn.Conv2d(in_channels, out_channels, 3, stride=1, padding=1, bias=False)\n            )\n            self.conv2 = nn.utils.spectral_norm(\n                nn.Conv2d(out_channels, out_channels, 3, stride=1, padding=1, bias=False)\n            )\n        else:\n            self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride=1, padding=1, bias=False)\n            self.conv2 = nn.Conv2d(out_channels, out_channels, 3, stride=1, padding=1, bias=False)\n        \n        # Squeeze-and-Excitation block\n        self.se = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(out_channels, out_channels // 16, 1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels // 16, out_channels, 1),\n            nn.Sigmoid()\n        )\n        \n        # Residual connection\n        self.shortcut = None\n        if in_channels != out_channels:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, 1, stride=1, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n\n    def forward(self, x):\n        residual = x\n        \n        out = self.conv1(x)\n        out = self.norm1(out)\n        out = self.activation(out)\n        \n        out = self.conv2(out)\n        out = self.norm2(out)\n        \n        # Apply squeeze-and-excitation\n        se_weight = self.se(out)\n        out = out * se_weight\n        \n        if self.shortcut is not None:\n            residual = self.shortcut(residual)\n            \n        out += residual\n        out = self.activation(out)\n        \n        return out\n\nclass Generator(nn.Module):\n    def __init__(self, latent_dim, img_channels, features_g=64):\n        super(Generator, self).__init__()\n        self.init_size = 4\n        self.latent_dim = latent_dim\n        \n        # Calculate proper size for initial dense layer\n        # features_g * 16 represents the number of feature maps in the first layer\n        self.initial = nn.Sequential(\n            # Changed dimension to match the expected size\n            nn.Linear(latent_dim, features_g * 16 * self.init_size * self.init_size),\n            nn.LeakyReLU(0.2, inplace=True)\n        )\n        \n        # Main generation blocks\n        self.main = nn.ModuleList([\n            # 4x4 -> 8x8\n            nn.Sequential(\n                AdaptiveResidualBlock(features_g * 16, features_g * 8),\n                nn.Upsample(scale_factor=2),\n                SelfAttention(features_g * 8)\n            ),\n            # 8x8 -> 16x16\n            nn.Sequential(\n                AdaptiveResidualBlock(features_g * 8, features_g * 4),\n                nn.Upsample(scale_factor=2),\n                SelfAttention(features_g * 4)\n            ),\n            # 16x16 -> 32x32\n            nn.Sequential(\n                AdaptiveResidualBlock(features_g * 4, features_g * 2),\n                nn.Upsample(scale_factor=2)\n            ),\n            # 32x32 -> 64x64\n            nn.Sequential(\n                AdaptiveResidualBlock(features_g * 2, features_g),\n                nn.Upsample(scale_factor=2)\n            )\n        ])\n        \n        # Final layers\n        self.final = nn.Sequential(\n            nn.Conv2d(features_g, img_channels, 3, stride=1, padding=1),\n            nn.Tanh()\n        )\n        \n        # Initialize weights\n        self.apply(self._init_weights)\n        \n    def _init_weights(self, m):\n        if isinstance(m, (nn.Conv2d, nn.Linear)):\n            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n                \n    def forward(self, z):\n        # Initial dense layer\n        out = self.initial(z)\n        # Reshape using the correct dimensions\n        out = out.view(-1, 64 * 16, self.init_size, self.init_size)\n        \n        # Main generation blocks\n        for block in self.main:\n            out = block(out)\n        \n        # Final convolution\n        img = self.final(out)\n        return img\n\n\n# Critic Model (Discriminator)\nclass Critic(nn.Module):\n    def __init__(self, img_channels, features_d=64):\n        super(Critic, self).__init__()\n        \n        def critic_block(in_channels, out_channels, normalize=True):\n            layers = [nn.utils.spectral_norm(\n                nn.Conv2d(in_channels, out_channels, 4, stride=2, padding=1)\n            )]\n            if normalize:\n                layers.append(nn.InstanceNorm2d(out_channels, affine=True))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n        \n        self.features_d = features_d\n        \n        self.initial = nn.Sequential(\n            *critic_block(img_channels, features_d, normalize=False),\n            SelfAttention(features_d)\n        )\n        \n        self.main = nn.Sequential(\n            *critic_block(features_d, features_d * 2),\n            SelfAttention(features_d * 2),\n            *critic_block(features_d * 2, features_d * 4),\n            SelfAttention(features_d * 4),\n            *critic_block(features_d * 4, features_d * 8),\n        )\n        \n        # Calculate the correct input size for the final linear layer\n        # After 4 downsampling layers (stride=2), the spatial dimensions are reduced by factor of 16\n        # For 64x64 input: 64/16 = 4, so final spatial dimensions are 4x4\n        final_spatial_size = 4\n        flattened_size = features_d * 8 * final_spatial_size * final_spatial_size\n        \n        self.final = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(flattened_size, 1)\n        )\n        \n        self.apply(self._init_weights)\n        \n    def _init_weights(self, m):\n        if isinstance(m, (nn.Conv2d, nn.Linear)):\n            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n    \n    def forward(self, img):\n        features = self.initial(img)\n        features = self.main(features)\n        return self.final(features)\n\n# Gradient Penalty Function\ndef compute_gradient_penalty(critic, real_imgs, fake_imgs, device):\n    \"\"\"Compute the gradient penalty for WGAN-GP.\"\"\"\n    alpha = torch.rand(real_imgs.size(0), 1, 1, 1).to(device)\n    interpolates = (alpha * real_imgs + (1 - alpha) * fake_imgs).requires_grad_(True)\n    d_interpolates = critic(interpolates)\n    fake = torch.ones(real_imgs.size(0), 1).to(device)\n\n    gradients = torch.autograd.grad(\n        outputs=d_interpolates,\n        inputs=interpolates,\n        grad_outputs=fake,\n        create_graph=True,\n        retain_graph=True,\n        only_inputs=True,\n    )[0]\n\n    gradients = gradients.view(gradients.size(0), -1)\n    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n    return gradient_penalty\n\n# WGAN Training\ndef train_wgan(generator, critic, dataloader, latent_dim, epochs, sample_interval=1, lambda_gp=10):\n    optimizer_G = optim.Adam(generator.parameters(), lr=1e-4, betas=(0.0, 0.999))\n    optimizer_C = optim.Adam(critic.parameters(), lr=4e-4, betas=(0.0, 0.999))\n\n    g_losses, c_losses, ssim_scores = [], [], []\n\n    for epoch in range(epochs):\n        g_loss_epoch, c_loss_epoch = 0.0, 0.0\n        epoch_ssim_scores = []\n\n        for i, imgs in enumerate(tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{epochs}\")):\n            real_imgs = imgs.to(device).float()\n            batch_size = real_imgs.size(0)\n\n            # Train Critic\n            optimizer_C.zero_grad()\n            z = torch.randn(batch_size, latent_dim).to(device)\n            fake_imgs = generator(z)\n            real_loss = torch.mean(critic(real_imgs))\n            fake_loss = torch.mean(critic(fake_imgs))\n            gradient_penalty = compute_gradient_penalty(critic, real_imgs, fake_imgs, device)\n            c_loss = fake_loss - real_loss + lambda_gp * gradient_penalty\n            c_loss.backward()\n            optimizer_C.step()\n            c_loss_epoch += c_loss.item()\n\n            # Train Generator every n_critic steps\n            if i % 5 == 0:\n                optimizer_G.zero_grad()\n                z = torch.randn(batch_size, latent_dim).to(device)\n                gen_imgs = generator(z)\n                g_loss = -torch.mean(critic(gen_imgs))\n                g_loss.backward()\n                optimizer_G.step()\n                g_loss_epoch += g_loss.item()\n\n                # Calculate SSIM periodically\n                ssim_score = calculate_ssim(real_imgs, gen_imgs)\n                epoch_ssim_scores.append(ssim_score)\n\n        # Record losses and SSIM\n        g_losses.append(g_loss_epoch / len(dataloader))\n        c_losses.append(c_loss_epoch / len(dataloader))\n        ssim_scores.append(np.mean(epoch_ssim_scores))\n\n        # Save sample images\n        if (epoch + 1) % sample_interval == 0:\n            display_sample_images(generator, latent_dim)\n\n        print(f\"[Epoch {epoch + 1}] Generator Loss: {g_losses[-1]:.4f}, Critic Loss: {c_losses[-1]:.4f}, SSIM: {ssim_scores[-1]:.4f}\")\n\n    # Plot losses and SSIM\n    plot_losses_and_ssim(g_losses, c_losses, ssim_scores)\n\n# Display and Plot Functions\ndef display_sample_images(generator, latent_dim):\n    generator.eval()\n    z = torch.randn(16, latent_dim).to(device)\n    gen_imgs = generator(z).detach().cpu()\n    gen_imgs = (gen_imgs + 1) / 2  # Rescale to [0, 1]\n    grid = make_grid(gen_imgs, nrow=4)\n    plt.imshow(grid.permute(1, 2, 0))\n    plt.axis(\"off\")\n    plt.show()\n    generator.train()\n\ndef plot_losses_and_ssim(g_losses, c_losses, ssim_scores):\n    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10))\n    \n    # Loss plot\n    ax1.plot(g_losses, label=\"Generator Loss\")\n    ax1.plot(c_losses, label=\"Critic Loss\")\n    ax1.set_xlabel(\"Epoch\")\n    ax1.set_ylabel(\"Loss\")\n    ax1.legend()\n    \n    # SSIM plot\n    ax2.plot(ssim_scores, label=\"SSIM Score\", color='green')\n    ax2.set_xlabel(\"Epoch\")\n    ax2.set_ylabel(\"SSIM\")\n    ax2.legend()\n    \n    plt.tight_layout()\n    plt.show()\n    \ndef calculate_ssim(real_imgs, gen_imgs, win_size=3):\n    \"\"\"\n    Calculate SSIM between real and generated images\n    \n    Args:\n    real_imgs (torch.Tensor): Real image batch\n    gen_imgs (torch.Tensor): Generated image batch\n    win_size (int): Window size for SSIM calculation (should be smaller than the image size)\n    \n    Returns:\n    float: Average SSIM score\n    \"\"\"\n    # Convert images to numpy for SSIM calculation\n    real_np = real_imgs.detach().cpu().numpy().transpose(0, 2, 3, 1)\n    gen_np = gen_imgs.detach().cpu().numpy().transpose(0, 2, 3, 1)\n    \n    # Normalize images to [0, 1] range\n    real_np = (real_np + 1) / 2\n    gen_np = (gen_np + 1) / 2\n    \n    # Compute SSIM for each image with custom window size and data_range specified\n    ssim_scores = [ssim_sklearn(real_np[i], gen_np[i], multichannel=True, win_size=win_size, data_range=1.0) \n                   for i in range(len(real_np))]\n    \n    return np.mean(ssim_scores)\n\n# Hyperparameters and Setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nlatent_dim = 128\nimg_channels = 3\nimg_size = 64\nepochs = 500\nbatch_size = 64\nlambda_gp = 10\n\ngenerator = Generator(latent_dim, img_channels).to(device)\ncritic = Critic(img_channels).to(device)\n\nprint(f\"Available GPUs: {torch.cuda.device_count()}\")\nif torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs\")\n    generator = nn.DataParallel(generator)\n    critic = nn.DataParallel(critic)\n\ndata_path = \"/kaggle/input/minecraft-10000-skins\"  # Replace with your dataset directory\ndataloader = load_minecraft_data(data_path, img_size=img_size, batch_size=batch_size)\n    \n    # Initialize and Train WGAN\ntrain_wgan(generator, critic, dataloader, latent_dim, epochs, sample_interval=5, lambda_gp=lambda_gp)\ntorch.save(generator.state_dict(), f\"generator_epoch_{epochs}.pth\")\ntorch.save(critic.state_dict(), f\"critic_epoch_{epochs}.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T16:11:53.756815Z","iopub.execute_input":"2025-01-29T16:11:53.757232Z","execution_failed":"2025-01-29T16:46:39.063Z"},"editable":false},"outputs":[{"name":"stdout","text":"Available GPUs: 2\nUsing 2 GPUs\nTotal valid images: 10000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/500:   0%|          | 0/157 [00:00<?, ?it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"\"\"\"import torch\nfrom torchvision.utils import save_image\nimport os\n\n# Load the saved model weights\ngenerator.load_state_dict(torch.load(\"/kaggle/input/epoch-500-0.0001-2x-db-15-sample-10000/pytorch/v1/1/generator_epoch_500.pth\", map_location=device))\ngenerator.eval()  # Set the generator to evaluation mode\n\n# Generate and save images individually\ndef generate_and_save_individual_images(generator, latent_dim, output_dir=\"generated_images\", num_images=16):\n    os.makedirs(output_dir, exist_ok=True)  # Create directory to save images\n    for i in range(num_images):\n        z = torch.randn(1, latent_dim).to(device)  # Generate a single latent vector\n        with torch.no_grad():\n            gen_img = generator(z)  # Generate a single image\n        gen_img = (gen_img + 1) / 2  # Rescale to [0, 1]\n        gen_img = gen_img.cpu()  # Move to CPU\n        save_path = os.path.join(output_dir, f\"generated_image_{i + 1}.png\")\n        save_image(gen_img, save_path)  # Save the individual image\n        print(f\"Generated image saved to {save_path}\")\n\n\n# Generate and save the images\ngenerate_and_save_individual_images(generator, latent_dim)\"\"\"","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-29T16:11:16.731Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"!pip install skinpy\"\"\"","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-29T16:11:16.731Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"import torch\nfrom skinpy import Skin, Perspective\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\ndef generate_and_render_skin(generator, latent_dim):\n    # Generate a single latent vector\n    z = torch.randn(1, latent_dim).to(device)\n    \n    with torch.no_grad():\n        gen_img = generator(z)  # Generate a single image\n    \n    # Rescale to [0, 1] and move to CPU\n    gen_img = (gen_img + 1) / 2\n    gen_img = gen_img.cpu().squeeze().permute(1, 2, 0)\n    \n    # Convert to PIL Image and add alpha channel\n    gen_img_pil = Image.fromarray((gen_img.numpy() * 255).astype('uint8'))\n    gen_img_rgba = gen_img_pil.convert('RGBA')\n    gen_img_rgba.save('generated_skin.png')\n    \n    # Create skin and perspective for rendering\n    skin = Skin.from_path(\"generated_skin.png\")\n    perspective = Perspective(\n        x=\"left\",\n        y=\"front\", \n        z=\"up\",\n        scaling_factor=5\n    )\n    \n    # Save and render the isometric image\n    render = skin.to_isometric_image(perspective)\n    render.save(\"render.png\")\n    \n    # Display images\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n    \n    # Display generated skin\n    ax1.imshow(gen_img_rgba)\n    ax1.set_title('Generated Skin')\n    ax1.axis('off')\n    \n    # Display rendered skin\n    ax2.imshow(render)\n    ax2.set_title('Isometric Render')\n    ax2.axis('off')\n    plt.tight_layout()\n    plt.show()\n\n# Generate and render the skin\nfor i in range(20):\n    generate_and_render_skin(generator, latent_dim)\"\"\"","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-29T16:11:16.732Z"},"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null}]}